# 函数微分算法的比较实验报告
## 一、实验目的
1. 掌握中心差分法和Richardson外推法计算函数导数的原理和实现方法
2. 通过数值实验比较两种数值微分方法的精度特性
3. 理解不同数值微分方法的适用场景和局限性
## 二、实验原理
### 2.1 待分析函数
$$f(x) = 1 + \frac{1}{2}\tanh(2x)$$

### 2.2 中心差分法
- 计算公式：
  $$f'(x)\approx \frac{f(x+h)-f(x-h)}{2h}$$
### 2.3 Richardson外推法
- 计算公式：
  $$D_{i,j} = \frac{4^j D_{i,j-1} - D_{i-1,j-1}}{4^j - 1},\quad D_{i,0}=\frac{f(x+h_i)-f(x-h_i)}{2h_i}$$
## 三、代码实现
（此处简要说明代码实现的主要步骤和或插入代码）
```# 定义函数 f(x) = 1 + 0.5 * tanh(2x)
def f(x):
    return 1 + 0.5 * np.tanh(2 * x)

# 使用 Sympy 获取解析导数函数
def get_analytical_derivative():
    x = symbols('x')
    expr = diff(1 + 0.5 * tanh(2 * x), x)
    return lambdify(x, expr)

# 中心差分法实现
def calculate_central_difference(x, f):
    dy = []
    for i in range(1, len(x)-1):
        h = x[i+1] - x[i]
        dy.append((f(x[i+1]) - f(x[i-1])) / (2 * h))
    return np.array(dy)

# Richardson 外推法实现
def richardson_derivative_all_orders(x, f, h, max_order=3):
    R = np.zeros((max_order + 1, max_order + 1))
    for i in range(max_order + 1):
        hi = h / (2**i)
        R[i, 0] = (f(x + hi) - f(x - hi)) / (2 * hi)
    
    for j in range(1, max_order + 1):
        for i in range(max_order - j + 1):
            R[i, j] = (4**j * R[i+1, j-1] - R[i, j-1]) / (4**j - 1)
    
    return [R[0, j] for j in range(1, max_order + 1)]

```

## 四、实验结果与分析
### 4.1 导数计算结果对比
（插入三种方法计算结果的对比图）

### 4.2 误差分析 
#### 4.2.1 中心差分法误差分析
（插入中心差分法误差随步长变化的log-log图及分析）
#### 4.2.2 Richardson外推法误差分析
（插入不同阶数Richardson外推法误差随步长变化的log-log图及分析）

## 五、实验讨论
### 5.1 两种方法的优缺点分析
1. 中心差分法
优点：
实现简单，计算效率高
适用于快速初步导数估算
对内存需求低
缺点：
精度有限（二阶精度）
对步长选择敏感，存在误差-稳定性权衡
当步长很小时，舍入误差会主导计算
2. Richardson外推法
优点：
显著提高导数计算精度
通过系统性外推减少截断误差
可灵活选择外推阶数平衡精度与计算成本
缺点：
实现相对复杂
计算量随外推阶数指数增长
对函数计算开销大的情况不友好
